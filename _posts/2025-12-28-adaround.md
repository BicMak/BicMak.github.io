---
title: Adaround
date: 2025-12-28 20:30:00 +0900
categories: [Quantization]
tags: [Quantization, pytorch]
math: true
---


# 1. introduction

$$
\hat{w} = s \cdot \text{clip}\left(\left\lfloor\frac{w}{s}\right\rceil, n, p\right)
$$

- 기존의 방법대로 레이어의 파라미터들을 rounding-to-nearest 대로 QSNR을 높게 가져 가능 방법으로 하니깐 학습이 제대로 되지않아서 adaround라는 방법을 제안함

- 기존의 양자화 방법은 각 레이어에 대해서는 최적합을 낼수 있지만 신경망 단위에서는 최적화가 되지 않는 문제가 있음

- 신경망의 모든 레이어를 고려해서 양자화를 할 때 올림과 내림을 결정해야될 필요가 있음

---

# 2. 이론적 배경
![alt text](/assets/img/2025-12-28-adaround/image-1.png)
- 기존에는 레이어 하나의 채널 한개에 대해서만 해석을 했음. 하지만 실제로는 레이어간의 가중치 로스를 분석 해보면 nearest rounding기법이 최적값이 아님
- 위의 scatter plot을 보면 확률적으로 라운딩을 결정했을때가 nearest 보다 정확도가 높은 경우가 있음

- 양자화 loss에 대해서 수식으로 나타내면

$$
E[L(x, y, \mathbf{w} + \Delta\mathbf{w}) - L(x, y, \mathbf{w})] \tag{2}

$$

- 저 위의 식에서 Loss를 최종적으로 정리하면 아래와 같이 표현이 가능함

$$
\Delta\mathbf{w}^T \cdot \mathbf{g}(\mathbf{w}) + \frac{1}{2}\Delta\mathbf{w}^T \cdot H(\mathbf{w}) \cdot \Delta\mathbf{w}
$$

- **1차 항 (그래디언트 항)**: 수렴된 네트워크에서는 ~0 → 무시 가능
- **2차 항 (Hessian 항)**: 여기가 핵심!

## 2.1 Hessian 행렬

- 해당 Hessian 행렬은 아래와 같이 표현하고

$$
\Delta\mathbf{w}^T \cdot H(\mathbf{w}) \cdot \Delta\mathbf{w}
$$

- 식을 풀면 아래와 같이 유도가 가능함.

```
= Δw₁² × H₁₁ + Δw₂² × H₂₂ + ... (대각 항)
  + 2×Δw₁×Δw₂ × H₁₂ + ... (비대각 항)

```

- 여기서 대각항 Δw₁² 은 이미 nearest에서 최적화를 해줘서 최소값을 가지게 되지만
- 비대각 항: Δw₁×Δw₂ 은 부호의 상호 작용이 중요해서 두 가중치의 오류 부호가 동일하게 되면 가중치가 증가
- 레이어가 많아질수록 비대각항의 loss 값의 누적이 커지게 되어 양자화 loss를 감소시키는 문제가 있어 해당부분을 고려해서 학습하기 위해 **Adaround** 가 등장하게 됨

---

# 3. Adaround
- Adaround(Adaptive round)는 양자화 후 개별 가중치에 대해서 올림과 내림여부를 결정하는 방법임
- 각 레이어마다 V라고 하는 가중치를 추가하여 시그모이드 출력으로 round 여부를 결정함

## 3.1 AdaRound 의 Loss function

$$
\arg \min_V \left|f_a(\mathbf{W}\hat{\mathbf{x}}) - f_a(\tilde{\mathbf{W}}\hat{\mathbf{x}}_f)\right|_F^2 + \lambda f{reg}(V)
$$



### 손실 함수 (Loss Function)

$$
\left|f_a(\mathbf{W}\hat{\mathbf{x}}) - f_a(\tilde{\mathbf{W}}\hat{\mathbf{x}}_f)\right|_F^2 + \lambda f_{reg}(V)
$$

**의미:**

- 원본 출력과 양자화된 출력의 차이
- 활성화 함수까지 고려
- **이것을 최소화하는 V를 찾는 게 전부**

###  정규화항 (Regularization)

$$
\lambda f_{reg}(V) = \lambda \sum_{i,j} (1 - |2h(V_{i,j}) - 1|)^\beta
$$

**의미:**

- h(V)를 0 또는 1로 강제


![alt text](/assets/img/2025-12-28-adaround/image-2.png)
- 그림과 같이 beta가 커질수록 정규화식의 값이 1에 가까운 구간이 넓어지는것을 확인이 가능함
- Annealing으로 β 감소, epoch 초기에는 beta값을 작게하여 시그모이드에 기울기를 완만하게 진행
- epoch가 뒤로 갈수록 이진값을 결정하고 loss를 수렴시키기 위해서 정규화값의 비중이 낮아지도록 beta값 감소
</aside>

## 3.2 구현의 핵심 3개 식

### 1. Rectified Sigmoid (식 23)

$$
h(V_{i,j}) = \text{clip}(\sigma(V_{i,j})(\zeta - \gamma) + \gamma, 0, 1)
$$

- h(V)를 0과 1 사이로 "압축"하기, 활성화 되는값이 0과 1이 넘지 않도록 강제로 고정

### 2. 소프트-양자화 (식 22)

$$
\tilde{\mathbf{W}} = s \cdot \text{clip}\left(\left\lfloor\frac{\mathbf{W}}{s}\right\rfloor + h(V), n, p\right)
$$

- 기존 모델에서 양자화는 이산식이기 때문에 경사하강법을 적용할수 없어서 이산값으로 변환

### 3. 정규화항 (식 24)

$$
f_{reg}(V) = \sum_{i,j} (1 - |2h(V_{i,j}) - 1|)^\beta
$$

- h(V)를 0 또는 1로 수렴이 되어야 하기 때문에 정규화 식으로 강제하는 식임.
- 트레이닝이 지날수록 정규화 값이 특정지점에서 수렴해야 되기 때문에 강제로 자유도를 beta로 제어


